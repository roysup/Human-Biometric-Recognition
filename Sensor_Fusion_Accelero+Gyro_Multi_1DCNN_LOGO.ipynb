{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVB1XA8LDc86"
      },
      "source": [
        "\n",
        "# **Deep-Learning Based Human Physical Activity Recognition with Wearable Sensor Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code implements gender recognition utilising wearable sensor data for multiple activities and sensor placements. This particular example utilises all the six measured signals (accelero x, y, z and gyro x, y, z) and applies end-to-end deep learning, fusing readings from multiple sensors for each activity via a multi-head 1D CNN."
      ],
      "metadata": {
        "id": "TfzLr9lp3fm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC3orRvYnMqI",
        "outputId": "034bf6d3-3831-46e8-933c-77a56308ed49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "%matplotlib inline\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import scipy.io\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.layers import Input, Add, Permute, Reshape, multiply\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from matplotlib import pyplot, image\n",
        "from scipy import stats\n",
        "from keras.layers.merge import concatenate\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJonGs4rZmOH",
        "outputId": "ec361d6d-70e6-4283-cbd9-99be6d396850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/HAR Research Project/Datasets\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/HAR Research Project/Datasets/Research Dataset')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUrRXDz757hP"
      },
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFHzaLSGdbe3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The following section reads the accelerometer and gyroscope sensor readings of several different activities conducted by test subjects into a Pandas data frame.\n",
        "The dataset consists of 135 subject-trial groups, with 45 subjects undergoing three trials each for seven different activities simultaneously measured by five sensors.\n",
        "The dataset has been divided into seven activity folders, whereby each folder consists of .CSV files named as per the following convention:\n",
        "'activityNumber_subjectName_trialNumber_startTimestamp_sensorNumber'\n",
        "'''\n",
        "\n",
        "archive_ = globals()\n",
        "files_ = globals()\n",
        "df_ = globals()\n",
        "\n",
        "archive = []\n",
        "all_dfs = []\n",
        "\n",
        "# Loop through all the seven activity folders\n",
        "for i in range(1,8):\n",
        "  folder_path = f\"{i}\" + '.zip'\n",
        "  # Read ZIP folder\n",
        "  archive_[f\"{i}\"] = ZipFile(folder_path, 'r')\n",
        "  archive.append(archive_[f\"{i}\"])\n",
        "  # Read names of the files in ZIP folder\n",
        "  files_[f\"{i}\"] = archive_[f\"{i}\"].namelist()\n",
        "  dfs = []\n",
        "  # Loop through all the files in the activity ZIP folder\n",
        "  for file in files_[f\"{i}\"][1:]: # skip .DS_Store\n",
        "    # Read file\n",
        "    frame = pd.read_csv(archive[i-1].open(file), header=None)\n",
        "    # Add filename as column\n",
        "    frame['filename'] = os.path.basename(file)\n",
        "    # Append all dataframes from within the activity\n",
        "    dfs.append(frame)\n",
        "  # Concatenate all dataframes from within the activity\n",
        "  df_[f\"{i}\"] = pd.concat(dfs,ignore_index=True)\n",
        "  # Append all dataframes from all activites\n",
        "  all_dfs.append(df_[f\"{i}\"])\n",
        "# Concatenate all dataframes from all activities\n",
        "raw_df = pd.concat(all_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the subject characteristics into a Pandas data frame.\n",
        "gender_df = pd.read_csv('subject_chars_sheet1.csv')"
      ],
      "metadata": {
        "id": "lJmGEZFvRORa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_MBTqaKbDTi"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9o1z1SD5ziV"
      },
      "outputs": [],
      "source": [
        "# Rename columns\n",
        "full_df = raw_df.rename(columns={0: 'x_accelero', 1: 'y_accelero', 2: 'z_accelero', 3 : 'x_gyro', 4: 'y_gyro', 5: 'z_gyro'}, errors=\"raise\")\n",
        "\n",
        "# Split the filename into seperate columns\n",
        "full_df[['activity','subject_name', 'trial_number', 'timestamp', 'sensor_position']] = full_df['filename'].str.split('_',expand=True)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "full_df[['sensor_position','file_type']] = full_df['sensor_position'].str.split('.',expand=True)\n",
        "\n",
        "# Merge the sensor readings' dataframe with the subject characteristcs' dataframe\n",
        "full_df.drop(columns=['filename', 'file_type'], inplace=True)\n",
        "gender_df = pd.read_csv('subject_chars_sheet1.csv')\n",
        "full_gd_df = full_df.merge(gender_df, on='subject_name', how='left')\n",
        "\n",
        "# Convert datatype for sensor readings\n",
        "full_gd_df['x_accelero'] = pd.to_numeric(full_gd_df['x_accelero'])\n",
        "full_gd_df['y_accelero'] = pd.to_numeric(full_gd_df['y_accelero'])\n",
        "full_gd_df['z_accelero'] = pd.to_numeric(full_gd_df['z_accelero'])\n",
        "\n",
        "full_gd_df['x_gyro'] = pd.to_numeric(full_gd_df['x_gyro'])\n",
        "\n",
        "full_gd_df['y_gyro'] = full_gd_df['y_gyro'].astype('string')\n",
        "full_gd_df['y_gyro'] = full_gd_df['y_gyro'].apply(lambda x: x[:-2] if x[-2:] == \".1\" else x)\n",
        "full_gd_df['y_gyro'] = pd.to_numeric(full_gd_df['y_gyro'])\n",
        "\n",
        "full_gd_df['z_gyro'] = full_gd_df['z_gyro'].astype('string')\n",
        "full_gd_df['z_gyro'] = full_gd_df['z_gyro'].apply(lambda x: x[:-2] if x[-2:] == \".1\" else x)\n",
        "full_gd_df['z_gyro'] = pd.to_numeric(full_gd_df['z_gyro'])\n",
        "\n",
        "full_gd_df['sensor_position'] = pd.to_numeric(full_gd_df['sensor_position'])\n",
        "full_gd_df['activity'] = pd.to_numeric(full_gd_df['activity'])\n",
        "full_gd_df['Gender Code'] = pd.to_numeric(full_gd_df['Gender Code'])\n",
        "\n",
        "full_gd_df[\"Subject_Trial_Number\"] = full_gd_df[\"subject_name\"] + full_gd_df[\"trial_number\"]\n",
        "full_gd_df[\"Subject_Trial_Sensor_Number\"] = full_gd_df[\"subject_name\"] + full_gd_df[\"trial_number\"] + full_gd_df[\"sensor_position\"].astype(str)\n",
        "\n",
        "# Merge subject name and trial number\n",
        "full_gd_df[\"Subject_Trial_Sensor_Activity_Number\"] = full_gd_df[\"subject_name\"] + full_gd_df[\"trial_number\"] + full_gd_df[\"sensor_position\"].astype(str) + full_gd_df[\"activity\"].astype(str)\n",
        "\n",
        "# This additional line drops subject records where the data is not given or is incomplete for all trials and sensors. \n",
        "# Unlike previous experiments where one subject_trial is left out for 1 sensor at a time, and rest are reshaped and computed, in this experiment sensors are used in parallel\n",
        "# Leave one out returns the indices, therefore if data is missing for a parallel sensor the method would return an error\n",
        "full_gd_df = full_gd_df[(full_gd_df.subject_name != 'Cat') & (full_gd_df.subject_name != 'cheryl') & (full_gd_df.subject_name != 'drdang') & (full_gd_df.subject_name != 'gdil') & (full_gd_df.subject_name != 'thanh') & (full_gd_df.subject_name != 'tuong')]\n",
        "full_gd_df = full_gd_df.reset_index(drop=True)\n",
        "full_gd_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WyHKCkJuQT9"
      },
      "outputs": [],
      "source": [
        "# The following code ensures all sensors have the same timesteps, before LOGO is called\n",
        "dfs = []\n",
        "activity_list = set(full_gd_df.Subject_Trial_Sensor_Activity_Number)\n",
        "for activity in activity_list:\n",
        "  partial_df = full_gd_df[full_gd_df.Subject_Trial_Sensor_Activity_Number == activity]\n",
        "  df_0 = pd.DataFrame([0]*(3000 - partial_df.shape[0]))\n",
        "  df = partial_df.append(df_0, ignore_index=True)\n",
        "  for column in ['Gender', 'Gender Code', 'Subject_Trial_Number', 'Subject_Trial_Sensor_Activity_Number', 'Subject_Trial_Sensor_Number', \n",
        "                 'activity', 'sensor_position', 'subject_name', 'trial_number']:\n",
        "                 df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "  df = df.replace(np.nan,0)\n",
        "  dfs.append(df)\n",
        "final_df = pd.concat(dfs,ignore_index=True)\n",
        "final_df = final_df.reset_index(drop=True)\n",
        "final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXTY8VOuYVX"
      },
      "source": [
        "## Function Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "jUsSmzqi_pBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfhjK-c53O1t"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch, lr):\n",
        "  ''''\n",
        "  Returns the initial learning rate for the first ten epochs and then decreases it exponentially afterward.\n",
        "  '''\n",
        "  if epoch < 10:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "# Create a callback for the Learning Rate Scheduler\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Confusion Matrix"
      ],
      "metadata": {
        "id": "J_a94vPi_rO2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3jULSfTuoon"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "    \n",
        "    Args:\n",
        "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "       class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "    \n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    # Normalize the confusion matrix.\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "    \n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJTr25A3R0L"
      },
      "source": [
        "## 2 Parallel Sensors "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "NqWF29tCY_TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-headed 1D CNN\n",
        "def cnn_model_creation(n_timesteps, n_features):\n",
        "    # head 1\n",
        "    inputs1 = Input(shape=(n_timesteps,n_features))\n",
        "    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs1) #change kernel size to 5 \n",
        "    bn1 = BatchNormalization()(conv1)\n",
        "    pool1 = GlobalAveragePooling1D()(bn1) \n",
        "    flat1 = Flatten()(pool1)\n",
        "\n",
        "    # head 2\n",
        "    inputs2 = Input(shape=(n_timesteps,n_features))\n",
        "    conv2 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs2)\n",
        "    bn2 = BatchNormalization()(conv2)\n",
        "    pool2 = GlobalAveragePooling1D()(bn2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "\n",
        "    # merge\n",
        "    merged = concatenate([flat1, flat2])\n",
        "\n",
        "    # interpretation\n",
        "    dense1 = Dense(16, activation='relu', kernel_regularizer='l2')(merged) #16\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "    # save a plot of the model\n",
        "    # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0-KPJu0nZDmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping Signal Sequences into Frames with the Sliding Window Method"
      ],
      "metadata": {
        "id": "dmsTaw-TZHw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaKObCBKXB4o"
      },
      "outputs": [],
      "source": [
        "def get_frames(X, Y):\n",
        "  \n",
        "  N_FEATURES = 6\n",
        "\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for Subject_Trial_Number_Encoded in set(X.Subject_Trial_Number_Encoded):  #for each group\n",
        "    current_frame = X.loc[X.Subject_Trial_Number_Encoded == Subject_Trial_Number_Encoded]  #get the all the frames for that group\n",
        "    start_index = min(current_frame.index)\n",
        "    end_index = max(current_frame.index) + 1\n",
        "    frame_size = len(current_frame)\n",
        "\n",
        "    ax = X['x_accelero'].values[start_index: end_index] \n",
        "    ay = X['y_accelero'].values[start_index: end_index]\n",
        "    az = X['z_accelero'].values[start_index: end_index]\n",
        "    gx = X['x_gyro'].values[start_index: end_index] \n",
        "    gy = X['y_gyro'].values[start_index: end_index]\n",
        "    gz = X['z_gyro'].values[start_index: end_index]\n",
        "\n",
        "    # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(Y[start_index: end_index])[0][0]\n",
        "    \n",
        "    frames.append([ax, ay, az, gx, gy, gz])\n",
        "    labels.append(label)\n",
        "  # returns frames of samples, each sample(group) with three features, each feature with n timesteps 28*3*1729 i.e. groups/samples * features * timesteps\n",
        "\n",
        "  # As the frame size differes for each group, the frames are padded\n",
        "  padded_frames = []\n",
        "  for row in frames:\n",
        "    shape = np.shape(row)\n",
        "    padded_array = np.zeros((6, 3000)) \n",
        "    padded_array[:shape[0],:shape[1]] = row\n",
        "    padded_frames.append(padded_array)\n",
        "\n",
        "  # return exactly the same shape as above but padded so each group has same number of steps - might not need this for this experiment! *CHECK*\n",
        "\n",
        "  # Bring the segments into a better shape\n",
        "  reshaped_padded_frames = np.transpose(padded_frames, (0, 2, 1))\n",
        "  reshaped_labels = np.asarray(labels)\n",
        "  \n",
        "  # finally converts the  28*3*1729  -> 28*1729*3 i.e. groups/samples * timestep * features\n",
        "  return reshaped_padded_frames, reshaped_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "fTE9G_ocZTsa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTznjrkNZOIN"
      },
      "outputs": [],
      "source": [
        "def get_scores(sensor_position_number, activity_number, full_gd_df): \n",
        "\n",
        "  partial_df = final_df.drop(columns=[0, \n",
        "                                    'subject_name', 'trial_number','timestamp', \n",
        "                                    'Age', 'Age Group', 'Gender', 'Weight', 'Height', 'BMI',\n",
        "                                    'Subject_Trial_Sensor_Activity_Number', 'Subject_Trial_Sensor_Number']) #DROP MORE COLUMNS\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  partial_df['Subject_Trial_Number_Encoded'] = le.fit_transform(partial_df['Subject_Trial_Number'])\n",
        "  partial_df = partial_df.drop(columns=['Subject_Trial_Number'])\n",
        "  partial_df_original = partial_df[partial_df[\"activity\"] == activity_number]\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 1\n",
        "  partial_df = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[0]]\n",
        "  partial_df = partial_df.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train = partial_df[partial_df['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test = partial_df[partial_df['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN = partial_df_train[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN = partial_df_train['Gender Code']\n",
        "  X_TRAIN = X_TRAIN.to_numpy() # for LOGO\n",
        "  y_TRAIN = y_TRAIN.to_numpy() # for LOGO\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 2\n",
        "  partial_df_2 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[1]]\n",
        "  partial_df_2 = partial_df_2.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_2 = partial_df_train_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_2 = partial_df_train_2['Gender Code']\n",
        "  X_TRAIN_2 = X_TRAIN_2.to_numpy() # for LOGO\n",
        "  y_TRAIN_2 = y_TRAIN_2.to_numpy() # for LOGO\n",
        "\n",
        "  # Both sensor 1 and 2 train sets should have same indices that can be referenced by logo\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST = partial_df_test[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST = partial_df_test['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test, reshaped_y_test = get_frames(X_TEST, y_TEST) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_2 = partial_df_test_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_2 = partial_df_test_2['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_2, reshaped_y_test_2 = get_frames(X_TEST_2, y_TEST_2) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  groups = partial_df_train['Subject_Trial_Number_Encoded']\n",
        "  logo = LeaveOneGroupOut()\n",
        "  split_number = logo.get_n_splits(X_TRAIN, y_TRAIN, groups)\n",
        "  groups = groups.to_numpy()\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "  # data for the confusion matrix\n",
        "  cm_holder_per_fold = []\n",
        "  # create empty lists for later\n",
        "  y_true, y_pred = list(), list()\n",
        "\n",
        "  for train_ix, val_ix in logo.split(X_TRAIN, y_TRAIN, groups):\n",
        "      # split data\n",
        "      X_train, X_val = X_TRAIN[train_ix, :], X_TRAIN[val_ix, :]\n",
        "      y_train, y_val = y_TRAIN[train_ix], y_TRAIN[val_ix] \n",
        "\n",
        "      X_train = pd.DataFrame(data = X_train, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val = pd.DataFrame(data = X_val, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train = pd.DataFrame(data = y_train)\n",
        "      y_val = pd.DataFrame(data = y_val) \n",
        "\n",
        "      reshaped_X_train, reshaped_y_train = get_frames(X_train, y_train) \n",
        "      reshaped_X_val, reshaped_y_val = get_frames(X_val, y_val) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_2, X_val_2 = X_TRAIN_2[train_ix, :], X_TRAIN_2[val_ix, :]\n",
        "      y_train_2, y_val_2 = y_TRAIN_2[train_ix], y_TRAIN_2[val_ix] \n",
        "\n",
        "      X_train_2 = pd.DataFrame(data = X_train_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val_2 = pd.DataFrame(data = X_val_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train_2 = pd.DataFrame(data = y_train_2)\n",
        "      y_val_2 = pd.DataFrame(data = y_val_2) \n",
        "\n",
        "      reshaped_X_train_2, reshaped_y_train_2 = get_frames(X_train_2, y_train_2) \n",
        "      reshaped_X_val_2, reshaped_y_val_2 = get_frames(X_val_2, y_val_2) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "      \n",
        "      # defining some input variables\n",
        "      n_timesteps, n_features, n_outputs = reshaped_X_train.shape[1], reshaped_X_val.shape[2], reshaped_y_train.shape[1]\n",
        "\n",
        "      # getting the model\n",
        "      model = cnn_model_creation(n_timesteps, n_features)\n",
        "\n",
        "      #model.summary()\n",
        "      #plot_model(model, to_file='model_plot_3.png', show_shapes=True, show_layer_names=True) \n",
        "\n",
        "      # fit model\n",
        "      history = model.fit([reshaped_X_train,reshaped_X_train_2], reshaped_y_train,\n",
        "                epochs=50,\n",
        "                verbose=0,\n",
        "                callbacks=[lr_callback],\n",
        "                validation_data=([reshaped_X_val,reshaped_X_val_2], reshaped_y_val))\n",
        "      \n",
        "      test_scores = model.evaluate([reshaped_X_test, reshaped_X_test_2],reshaped_y_test, verbose=0)\n",
        "      acc_per_fold.append(test_scores[1] * 100)\n",
        "      loss_per_fold.append(test_scores[0])\n",
        "\n",
        "      # Use the model to predict the values from the test data.\n",
        "      predictions_ = model.predict([reshaped_X_test, reshaped_X_test_2])\n",
        "      # Take the class with the highest probability from the test predictions\n",
        "      predictions = np.where(predictions_ > 0.5, 1, 0)\n",
        "      y_pred.append(predictions)\n",
        "      # Calculate the confusion matrix using sklearn.metrics\n",
        "      cm = metrics.confusion_matrix(reshaped_y_test, predictions)\n",
        "      \n",
        "      # append the confusion matrix of this fold\n",
        "      cm_holder_per_fold.append(cm)\n",
        "\n",
        "      # store ground truth and predicted values\n",
        "      y_true.append(reshaped_y_test) #[0])\n",
        "      y_pred.append(predictions) #[0])\n",
        "\n",
        "  class_names = ['0', '1']\n",
        "\n",
        "  # confusion matrix per fold\n",
        "  sum_cm_holder_per_fold = []\n",
        "  cm_shape = np.array([len(class_names),len(class_names)])\n",
        "  for k in range(len(cm_holder_per_fold)):\n",
        "      cm_mask = np.zeros(cm_shape)\n",
        "      cm_mask[:cm_holder_per_fold[k].shape[0], :cm_holder_per_fold[k].shape[1]] = cm_holder_per_fold[k]\n",
        "      sum_cm_holder_per_fold.append(cm_mask)\n",
        "\n",
        "  sum_cm_per_fold = sum(sum_cm_holder_per_fold)\n",
        "  figure = plot_confusion_matrix(sum_cm_per_fold, class_names=class_names)\n",
        "  plt.show()  \n",
        "  \n",
        "  mean_accuracy = np.mean(acc_per_fold)\n",
        "  mean_std = np.std(acc_per_fold)\n",
        "  mean_loss = np.mean(loss_per_fold)\n",
        "  print('Sensor:', sensor_position_number, 'Activity:', activity_number)\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  return mean_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the Train Function"
      ],
      "metadata": {
        "id": "yhp2A7rcZQWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz26H3Ii28GQ"
      },
      "outputs": [],
      "source": [
        "test_scores = []\n",
        "activity_list = [1, 2, 3, 4, 5, 6, 7]\n",
        "sensor_list = [\n",
        "               [1,2]\n",
        "              ,[1,3]\n",
        "              ,[1,4]\n",
        "              ,[1,5]\n",
        "              ,[2,3]\n",
        "              ,[2,4]\n",
        "              ,[2,5]\n",
        "              ,[3,4]\n",
        "              ,[3,5]\n",
        "              ,[4,5]\n",
        "               ]\n",
        "for activity in activity_list:\n",
        "  for sensor in sensor_list:\n",
        "    mean_accuracy = get_scores(sensor, activity, final_df)     \n",
        "    test_scores.append((activity, sensor, mean_accuracy))\n",
        "\n",
        "test_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the ROC-AUC Graphs"
      ],
      "metadata": {
        "id": "s7L-6pWKahtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr1, tpr1, thresh1 = roc_curve(reshaped_y_test, predictions_, pos_label=1)\n",
        "\n",
        "# roc curve for tpr = fpr \n",
        "random_probs = [0 for i in range(len(reshaped_y_test))]\n",
        "p_fpr, p_tpr, _ = roc_curve(reshaped_y_test, random_probs, pos_label=1)\n",
        "\n",
        "# matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "# plot roc curves\n",
        "plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='1D CNN')\n",
        "plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
        "# title\n",
        "plt.title('ROC curve')\n",
        "# x label\n",
        "plt.xlabel('False Positive Rate')\n",
        "# y label\n",
        "plt.ylabel('True Positive rate')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('ROC',dpi=300)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "L20X5mhyYlhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2VAYa_53dq8"
      },
      "source": [
        "## 3 Parallel Sensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "u9ws5m4fZxYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-headed 1D CNN\n",
        "def cnn_model_creation(n_timesteps, n_features):\n",
        "    # head 1\n",
        "    inputs1 = Input(shape=(n_timesteps,n_features))\n",
        "    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs1)\n",
        "    bn1 = BatchNormalization()(conv1)\n",
        "    pool1 = GlobalAveragePooling1D()(bn1) \n",
        "    flat1 = Flatten()(pool1)\n",
        "\n",
        "    # head 2\n",
        "    inputs2 = Input(shape=(n_timesteps,n_features))\n",
        "    conv2 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs2)\n",
        "    bn2 = BatchNormalization()(conv2)\n",
        "    pool2 = GlobalAveragePooling1D()(bn2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "\n",
        "    # head 3\n",
        "    inputs3 = Input(shape=(n_timesteps,n_features))\n",
        "    conv3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs3)\n",
        "    bn3 = BatchNormalization()(conv3)\n",
        "    pool3 = GlobalAveragePooling1D()(bn3)\n",
        "    flat3 = Flatten()(pool3)\n",
        "\n",
        "    # merge\n",
        "    merged = concatenate([flat1,flat2,flat3])\n",
        "\n",
        "    # interpretation\n",
        "    dense1 = Dense(16, activation='relu')(merged) #16\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "    # save a plot of the model\n",
        "    # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "AfSQQeeNZgpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping Signal Sequences into Frames with the Sliding Window Method"
      ],
      "metadata": {
        "id": "ekIThI4GZ2Yj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqVj09ZM3gGR"
      },
      "outputs": [],
      "source": [
        "def get_frames(X, Y):\n",
        "  \n",
        "  N_FEATURES = 6\n",
        "\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for Subject_Trial_Number_Encoded in set(X.Subject_Trial_Number_Encoded):  #for each group\n",
        "    current_frame = X.loc[X.Subject_Trial_Number_Encoded == Subject_Trial_Number_Encoded]  #get the all the frames for that group\n",
        "    start_index = min(current_frame.index)\n",
        "    end_index = max(current_frame.index) + 1\n",
        "    frame_size = len(current_frame)\n",
        "\n",
        "    ax = X['x_accelero'].values[start_index: end_index] \n",
        "    ay = X['y_accelero'].values[start_index: end_index]\n",
        "    az = X['z_accelero'].values[start_index: end_index]\n",
        "    gx = X['x_gyro'].values[start_index: end_index] \n",
        "    gy = X['y_gyro'].values[start_index: end_index]\n",
        "    gz = X['z_gyro'].values[start_index: end_index]\n",
        "\n",
        "    # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(Y[start_index: end_index])[0][0]\n",
        "    \n",
        "    frames.append([ax, ay, az, gx, gy, gz])\n",
        "    labels.append(label)\n",
        "  # returns frames of samples, each sample(group) with three features, each feature with n timesteps 28*3*1729 i.e. groups/samples * features * timesteps\n",
        "\n",
        "  # As the frame size differes for each group, the frames are padded\n",
        "  padded_frames = []\n",
        "  for row in frames:\n",
        "    shape = np.shape(row)\n",
        "    padded_array = np.zeros((6, 3000)) \n",
        "    padded_array[:shape[0],:shape[1]] = row\n",
        "    padded_frames.append(padded_array)\n",
        "\n",
        "  # return exactly the same shape as above but padded so each group has same number of steps - might not need this for this experiment! *CHECK*\n",
        "\n",
        "  # Bring the segments into a better shape\n",
        "  reshaped_padded_frames = np.transpose(padded_frames, (0, 2, 1))\n",
        "  reshaped_labels = np.asarray(labels)\n",
        "  \n",
        "  # finally converts the  28*3*1729  -> 28*1729*3 i.e. groups/samples * timestep * features\n",
        "  return reshaped_padded_frames, reshaped_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "8YnitJTsaQDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUJYCzGZ3gMv"
      },
      "outputs": [],
      "source": [
        "def get_scores(sensor_position_number, activity_number, full_gd_df): \n",
        "\n",
        "  partial_df = final_df.drop(columns=[0, \n",
        "                                    'subject_name', 'trial_number','timestamp', \n",
        "                                    'Age', 'Age Group', 'Gender', 'Weight', 'Height', 'BMI',\n",
        "                                    'Subject_Trial_Sensor_Activity_Number', 'Subject_Trial_Sensor_Number']) #DROP MORE COLUMNS\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  partial_df['Subject_Trial_Number_Encoded'] = le.fit_transform(partial_df['Subject_Trial_Number'])\n",
        "  partial_df = partial_df.drop(columns=['Subject_Trial_Number'])\n",
        "  partial_df_original = partial_df[partial_df[\"activity\"] == activity_number]\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 1\n",
        "  partial_df = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[0]]\n",
        "  partial_df = partial_df.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train = partial_df[partial_df['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test = partial_df[partial_df['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN = partial_df_train[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN = partial_df_train['Gender Code']\n",
        "  X_TRAIN = X_TRAIN.to_numpy() # for LOGO\n",
        "  y_TRAIN = y_TRAIN.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST = partial_df_test[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST = partial_df_test['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test, reshaped_y_test = get_frames(X_TEST, y_TEST) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 2\n",
        "  partial_df_2 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[1]]\n",
        "  partial_df_2 = partial_df_2.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_2 = partial_df_train_2[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_2 = partial_df_train_2['Gender Code']\n",
        "  X_TRAIN_2 = X_TRAIN_2.to_numpy() # for LOGO\n",
        "  y_TRAIN_2 = y_TRAIN_2.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_2 = partial_df_test_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_2 = partial_df_test_2['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_2, reshaped_y_test_2 = get_frames(X_TEST_2, y_TEST_2) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 3\n",
        "  partial_df_3 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[2]]\n",
        "  partial_df_3 = partial_df_3.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_3[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_3 = partial_df_3[partial_df_3['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_3 = partial_df_3[partial_df_3['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_3 = partial_df_train_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_3 = partial_df_train_3['Gender Code']\n",
        "  X_TRAIN_3 = X_TRAIN_3.to_numpy() # for LOGO\n",
        "  y_TRAIN_3 = y_TRAIN_3.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_3 = partial_df_test_3[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_3 = partial_df_test_3['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_3, reshaped_y_test_3 = get_frames(X_TEST_3, y_TEST_3) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  groups = partial_df_train['Subject_Trial_Number_Encoded']\n",
        "  logo = LeaveOneGroupOut()\n",
        "  split_number = logo.get_n_splits(X_TRAIN, y_TRAIN, groups)\n",
        "  groups = groups.to_numpy()\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "\n",
        "  for train_ix, val_ix in logo.split(X_TRAIN, y_TRAIN, groups):\n",
        "      # split data\n",
        "      X_train, X_val = X_TRAIN[train_ix, :], X_TRAIN[val_ix, :]\n",
        "      y_train, y_val = y_TRAIN[train_ix], y_TRAIN[val_ix] \n",
        "\n",
        "      X_train = pd.DataFrame(data = X_train, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val = pd.DataFrame(data = X_val, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train = pd.DataFrame(data = y_train)\n",
        "      y_val = pd.DataFrame(data = y_val) \n",
        "\n",
        "      reshaped_X_train, reshaped_y_train = get_frames(X_train, y_train) \n",
        "      reshaped_X_val, reshaped_y_val = get_frames(X_val, y_val) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_2, X_val_2 = X_TRAIN_2[train_ix, :], X_TRAIN_2[val_ix, :]\n",
        "      y_train_2, y_val_2 = y_TRAIN_2[train_ix], y_TRAIN_2[val_ix] \n",
        "\n",
        "      X_train_2 = pd.DataFrame(data = X_train_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val_2 = pd.DataFrame(data = X_val_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train_2 = pd.DataFrame(data = y_train_2)\n",
        "      y_val_2 = pd.DataFrame(data = y_val_2) \n",
        "\n",
        "      reshaped_X_train_2, reshaped_y_train_2 = get_frames(X_train_2, y_train_2) \n",
        "      reshaped_X_val_2, reshaped_y_val_2 = get_frames(X_val_2, y_val_2) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_3, X_val_3 = X_TRAIN_3[train_ix, :], X_TRAIN_3[val_ix, :]\n",
        "      y_train_3, y_val_3 = y_TRAIN_3[train_ix], y_TRAIN_3[val_ix] \n",
        "\n",
        "      X_train_3 = pd.DataFrame(data = X_train_3, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val_3 = pd.DataFrame(data = X_val_3, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train_3 = pd.DataFrame(data = y_train_3)\n",
        "      y_val_3 = pd.DataFrame(data = y_val_3) \n",
        "\n",
        "      reshaped_X_train_3, reshaped_y_train_3 = get_frames(X_train_3, y_train_3) \n",
        "      reshaped_X_val_3, reshaped_y_val_3 = get_frames(X_val_3, y_val_3) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # defining some input variables\n",
        "      n_timesteps, n_features, n_outputs = reshaped_X_train.shape[1], reshaped_X_val.shape[2], reshaped_y_train.shape[1]\n",
        "\n",
        "      # getting the model\n",
        "      model = cnn_model_creation(n_timesteps, n_features)\n",
        "\n",
        "      # fit model\n",
        "      history = model.fit([reshaped_X_train,reshaped_X_train_2,reshaped_X_train_3], reshaped_y_train,\n",
        "                epochs=50,\n",
        "                verbose=0,\n",
        "                callbacks=[lr_callback],\n",
        "                validation_data=([reshaped_X_val,reshaped_X_val_2,reshaped_X_val_3], reshaped_y_val)) # Chnage number of epochs!\n",
        "      \n",
        "      test_scores = model.evaluate([reshaped_X_test, reshaped_X_test_2,reshaped_X_test_3],reshaped_y_test, verbose=0)\n",
        "      acc_per_fold.append(test_scores[1] * 100)\n",
        "      loss_per_fold.append(test_scores[0])\n",
        "  mean_accuracy = np.mean(acc_per_fold)\n",
        "  mean_std = np.std(acc_per_fold)\n",
        "  mean_loss = np.mean(loss_per_fold)\n",
        "  print('Sensor:', sensor_position_number, 'Activity:', activity_number)\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  return mean_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the Train Function"
      ],
      "metadata": {
        "id": "nG0OTSEEaC8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmIOvBoEtiGr"
      },
      "outputs": [],
      "source": [
        "test_scores = []\n",
        "activity_list = [1, 2, 3, 4, 5, 6, 7]\n",
        "sensor_list = [\n",
        "               [1,2,3]\n",
        "              ,[1,2,4]\n",
        "              ,[1,2,5]\n",
        "              ,[1,3,4]\n",
        "              ,[1,3,5]\n",
        "              ,[1,4,5]\n",
        "              ,[2,3,4]\n",
        "              ,[2,3,5]\n",
        "              ,[2,4,5]\n",
        "              ,[3,4,5]\n",
        "               ]\n",
        "for activity in activity_list:\n",
        "  for sensor in sensor_list:\n",
        "    mean_accuracy = get_scores(sensor, activity, final_df)     \n",
        "    test_scores.append((activity, sensor, mean_accuracy))\n",
        "\n",
        "test_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0bBVmtY3gqN"
      },
      "source": [
        "## 4 Parallel Sensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "PaAm_jzLZznV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-headed 1D CNN\n",
        "def cnn_model_creation(n_timesteps, n_features):\n",
        "    # head 1\n",
        "    inputs1 = Input(shape=(n_timesteps,n_features))\n",
        "    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs1)\n",
        "    bn1 = BatchNormalization()(conv1)\n",
        "    pool1 = GlobalAveragePooling1D()(bn1) \n",
        "    flat1 = Flatten()(pool1)\n",
        "\n",
        "    # head 2\n",
        "    inputs2 = Input(shape=(n_timesteps,n_features))\n",
        "    conv2 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs2)\n",
        "    bn2 = BatchNormalization()(conv2)\n",
        "    pool2 = GlobalAveragePooling1D()(bn2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "\n",
        "    # head 3\n",
        "    inputs3 = Input(shape=(n_timesteps,n_features))\n",
        "    conv3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs3)\n",
        "    bn3 = BatchNormalization()(conv3)\n",
        "    pool3 = GlobalAveragePooling1D()(bn3)\n",
        "    flat3 = Flatten()(pool3)\n",
        "\n",
        "    # head 4\n",
        "    inputs4 = Input(shape=(n_timesteps,n_features))\n",
        "    conv4 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs4)\n",
        "    bn4 = BatchNormalization()(conv4)\n",
        "    pool4 = GlobalAveragePooling1D()(bn4)\n",
        "    flat4 = Flatten()(pool4)\n",
        "\n",
        "    # merge\n",
        "    merged = concatenate([flat1,flat2,flat3,flat4])\n",
        "\n",
        "    # interpretation\n",
        "    dense1 = Dense(16, activation='relu')(merged) #16\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3, inputs4], outputs=outputs)\n",
        "\n",
        "    # save a plot of the model\n",
        "    # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QY58nEeIZmrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping Signal Sequences into Frames with the Sliding Window Method"
      ],
      "metadata": {
        "id": "B5sGW-bkZ3Zj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAK9D-Mp3iU4"
      },
      "outputs": [],
      "source": [
        "def get_frames(X, Y):\n",
        "  \n",
        "  N_FEATURES = 6\n",
        "\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for Subject_Trial_Number_Encoded in set(X.Subject_Trial_Number_Encoded):  #for each group\n",
        "    current_frame = X.loc[X.Subject_Trial_Number_Encoded == Subject_Trial_Number_Encoded]  #get the all the frames for that group\n",
        "    start_index = min(current_frame.index)\n",
        "    end_index = max(current_frame.index) + 1\n",
        "    frame_size = len(current_frame)\n",
        "\n",
        "    ax = X['x_accelero'].values[start_index: end_index] \n",
        "    ay = X['y_accelero'].values[start_index: end_index]\n",
        "    az = X['z_accelero'].values[start_index: end_index]\n",
        "    gx = X['x_gyro'].values[start_index: end_index] \n",
        "    gy = X['y_gyro'].values[start_index: end_index]\n",
        "    gz = X['z_gyro'].values[start_index: end_index]\n",
        "\n",
        "    # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(Y[start_index: end_index])[0][0]\n",
        "    \n",
        "    frames.append([ax, ay, az, gx, gy, gz])\n",
        "    labels.append(label)\n",
        "  # returns frames of samples, each sample(group) with three features, each feature with n timesteps 28*3*1729 i.e. groups/samples * features * timesteps\n",
        "\n",
        "  # As the frame size differes for each group, the frames are padded\n",
        "  padded_frames = []\n",
        "  for row in frames:\n",
        "    shape = np.shape(row)\n",
        "    padded_array = np.zeros((6, 3000)) \n",
        "    padded_array[:shape[0],:shape[1]] = row\n",
        "    padded_frames.append(padded_array)\n",
        "\n",
        "  # return exactly the same shape as above but padded so each group has same number of steps - might not need this for this experiment! *CHECK*\n",
        "\n",
        "  # Bring the segments into a better shape\n",
        "  reshaped_padded_frames = np.transpose(padded_frames, (0, 2, 1))\n",
        "  reshaped_labels = np.asarray(labels)\n",
        "  \n",
        "  # finally converts the  28*3*1729  -> 28*1729*3 i.e. groups/samples * timestep * features\n",
        "  return reshaped_padded_frames, reshaped_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "-m5kbQXzZ79x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFiKDxJl3iZR"
      },
      "outputs": [],
      "source": [
        "def get_scores(sensor_position_number, activity_number, full_gd_df): \n",
        "\n",
        "  partial_df = final_df.drop(columns=[0, \n",
        "                                    'subject_name', 'trial_number','timestamp', \n",
        "                                    'Age', 'Age Group', 'Gender', 'Weight', 'Height', 'BMI',\n",
        "                                    'Subject_Trial_Sensor_Activity_Number', 'Subject_Trial_Sensor_Number']) #DROP MORE COLUMNS\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  partial_df['Subject_Trial_Number_Encoded'] = le.fit_transform(partial_df['Subject_Trial_Number'])\n",
        "  partial_df = partial_df.drop(columns=['Subject_Trial_Number'])\n",
        "  partial_df_original = partial_df[partial_df[\"activity\"] == activity_number]\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 1\n",
        "  partial_df = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[0]]\n",
        "  partial_df = partial_df.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train = partial_df[partial_df['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test = partial_df[partial_df['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN = partial_df_train[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN = partial_df_train['Gender Code']\n",
        "  X_TRAIN = X_TRAIN.to_numpy() # for LOGO\n",
        "  y_TRAIN = y_TRAIN.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST = partial_df_test[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST = partial_df_test['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test, reshaped_y_test = get_frames(X_TEST, y_TEST) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 2\n",
        "  partial_df_2 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[1]]\n",
        "  partial_df_2 = partial_df_2.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_2 = partial_df_train_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_2 = partial_df_train_2['Gender Code']\n",
        "  X_TRAIN_2 = X_TRAIN_2.to_numpy() # for LOGO\n",
        "  y_TRAIN_2 = y_TRAIN_2.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_2 = partial_df_test_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_2 = partial_df_test_2['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_2, reshaped_y_test_2 = get_frames(X_TEST_2, y_TEST_2) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 3\n",
        "  partial_df_3 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[2]]\n",
        "  partial_df_3 = partial_df_3.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_3 = partial_df_3[partial_df_3['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_3 = partial_df_3[partial_df_3['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_3 = partial_df_train_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_3 = partial_df_train_3['Gender Code']\n",
        "  X_TRAIN_3 = X_TRAIN_3.to_numpy() # for LOGO\n",
        "  y_TRAIN_3 = y_TRAIN_3.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_3 = partial_df_test_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_3 = partial_df_test_3['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_3, reshaped_y_test_3 = get_frames(X_TEST_3, y_TEST_3) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 4\n",
        "  partial_df_4 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[3]]\n",
        "  partial_df_4 = partial_df_4.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_4 = partial_df_4[partial_df_4['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_4 = partial_df_4[partial_df_4['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_4 = partial_df_train_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_4 = partial_df_train_4['Gender Code']\n",
        "  X_TRAIN_4 = X_TRAIN_4.to_numpy() # for LOGO\n",
        "  y_TRAIN_4 = y_TRAIN_4.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_4 = partial_df_test_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_4 = partial_df_test_4['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_4, reshaped_y_test_4 = get_frames(X_TEST_4, y_TEST_4) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  groups = partial_df_train['Subject_Trial_Number_Encoded']\n",
        "  logo = LeaveOneGroupOut()\n",
        "  split_number = logo.get_n_splits(X_TRAIN, y_TRAIN, groups)\n",
        "  groups = groups.to_numpy()\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "\n",
        "  for train_ix, val_ix in logo.split(X_TRAIN, y_TRAIN, groups):\n",
        "      # split data\n",
        "      X_train, X_val = X_TRAIN[train_ix, :], X_TRAIN[val_ix, :]\n",
        "      y_train, y_val = y_TRAIN[train_ix], y_TRAIN[val_ix] \n",
        "\n",
        "      X_train = pd.DataFrame(data = X_train, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val = pd.DataFrame(data = X_val, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train = pd.DataFrame(data = y_train)\n",
        "      y_val = pd.DataFrame(data = y_val) \n",
        "\n",
        "      reshaped_X_train, reshaped_y_train = get_frames(X_train, y_train) \n",
        "      reshaped_X_val, reshaped_y_val = get_frames(X_val, y_val) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_2, X_val_2 = X_TRAIN_2[train_ix, :], X_TRAIN_2[val_ix, :]\n",
        "      y_train_2, y_val_2 = y_TRAIN_2[train_ix], y_TRAIN_2[val_ix] \n",
        "\n",
        "      X_train_2 = pd.DataFrame(data = X_train_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro',  'Subject_Trial_Number_Encoded'])\n",
        "      X_val_2 = pd.DataFrame(data = X_val_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train_2 = pd.DataFrame(data = y_train_2)\n",
        "      y_val_2 = pd.DataFrame(data = y_val_2) \n",
        "\n",
        "      reshaped_X_train_2, reshaped_y_train_2 = get_frames(X_train_2, y_train_2) \n",
        "      reshaped_X_val_2, reshaped_y_val_2 = get_frames(X_val_2, y_val_2) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_3, X_val_3 = X_TRAIN_3[train_ix, :], X_TRAIN_3[val_ix, :]\n",
        "      y_train_3, y_val_3 = y_TRAIN_3[train_ix], y_TRAIN_3[val_ix] \n",
        "\n",
        "      X_train_3 = pd.DataFrame(data = X_train_3, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val_3 = pd.DataFrame(data = X_val_3, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train_3 = pd.DataFrame(data = y_train_3)\n",
        "      y_val_3 = pd.DataFrame(data = y_val_3) \n",
        "\n",
        "      reshaped_X_train_3, reshaped_y_train_3 = get_frames(X_train_3, y_train_3) \n",
        "      reshaped_X_val_3, reshaped_y_val_3 = get_frames(X_val_3, y_val_3) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "       # split data\n",
        "      X_train_4, X_val_4 = X_TRAIN_4[train_ix, :], X_TRAIN_4[val_ix, :]\n",
        "      y_train_4, y_val_4 = y_TRAIN_4[train_ix], y_TRAIN_4[val_ix] \n",
        "\n",
        "      X_train_4 = pd.DataFrame(data = X_train_4, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val_4 = pd.DataFrame(data = X_val_4, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      y_train_4 = pd.DataFrame(data = y_train_4)\n",
        "      y_val_4 = pd.DataFrame(data = y_val_4) \n",
        "\n",
        "      reshaped_X_train_4, reshaped_y_train_4 = get_frames(X_train_4, y_train_4) \n",
        "      reshaped_X_val_4, reshaped_y_val_4 = get_frames(X_val_4, y_val_4) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # defining some input variables\n",
        "      n_timesteps, n_features, n_outputs = reshaped_X_train.shape[1], reshaped_X_val.shape[2], reshaped_y_train.shape[1]\n",
        "\n",
        "      # getting the model\n",
        "      model = cnn_model_creation(n_timesteps, n_features)\n",
        "\n",
        "      # fit model\n",
        "      history = model.fit([reshaped_X_train,reshaped_X_train_2,reshaped_X_train_3,reshaped_X_train_4], reshaped_y_train,\n",
        "                epochs=50,\n",
        "                verbose=0,\n",
        "                callbacks=[lr_callback],\n",
        "                validation_data=([reshaped_X_val,reshaped_X_val_2,reshaped_X_val_3,reshaped_X_val_4], reshaped_y_val)) # Chnage number of epochs!\n",
        "      \n",
        "      test_scores = model.evaluate([reshaped_X_test, reshaped_X_test_2,reshaped_X_test_3,reshaped_X_test_4],reshaped_y_test, verbose=0)\n",
        "      acc_per_fold.append(test_scores[1] * 100)\n",
        "      loss_per_fold.append(test_scores[0])\n",
        "  mean_accuracy = np.mean(acc_per_fold)\n",
        "  mean_std = np.std(acc_per_fold)\n",
        "  mean_loss = np.mean(loss_per_fold)\n",
        "  print('Sensor:', sensor_position_number, 'Activity:', activity_number)\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  return mean_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the Train Function"
      ],
      "metadata": {
        "id": "5biaNBtQaGjj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8c3pTOO3idq"
      },
      "outputs": [],
      "source": [
        "test_scores = []\n",
        "activity_list = [1, 2, 3, 4, 5, 6, 7]\n",
        "sensor_list = [\n",
        "               [1,2,3,4]\n",
        "               ,[1,3,4,5]\n",
        "               ,[1,4,5,2]\n",
        "               ,[1,5,2,3]\n",
        "               ,[2,3,4,5]\n",
        "               ]\n",
        "for activity in activity_list:\n",
        "  for sensor in sensor_list:\n",
        "    mean_accuracy = get_scores(sensor, activity, final_df)     \n",
        "    test_scores.append((activity, sensor, mean_accuracy))\n",
        "\n",
        "test_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2esK0SeY3ird"
      },
      "source": [
        "## All 5 sensors in Parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "kHI6CsMfZ0IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-headed 1D CNN\n",
        "def cnn_model_creation(n_timesteps, n_features):\n",
        "    # head 1\n",
        "    inputs1 = Input(shape=(n_timesteps,n_features))\n",
        "    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs1)\n",
        "    bn1 = BatchNormalization()(conv1)\n",
        "    pool1 = GlobalAveragePooling1D()(bn1) \n",
        "    flat1 = Flatten()(pool1)\n",
        "\n",
        "    # head 2\n",
        "    inputs2 = Input(shape=(n_timesteps,n_features))\n",
        "    conv2 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs2)\n",
        "    bn2 = BatchNormalization()(conv2)\n",
        "    pool2 = GlobalAveragePooling1D()(bn2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "\n",
        "    # head 3\n",
        "    inputs3 = Input(shape=(n_timesteps,n_features))\n",
        "    conv3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs3)\n",
        "    bn3 = BatchNormalization()(conv3)\n",
        "    pool3 = GlobalAveragePooling1D()(bn3)\n",
        "    flat3 = Flatten()(pool3)\n",
        "\n",
        "    # head 4\n",
        "    inputs4 = Input(shape=(n_timesteps,n_features))\n",
        "    conv4 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs4)\n",
        "    bn4 = BatchNormalization()(conv4)\n",
        "    pool4 = GlobalAveragePooling1D()(bn4)\n",
        "    flat4 = Flatten()(pool4)\n",
        "\n",
        "    # head 5\n",
        "    inputs5 = Input(shape=(n_timesteps,n_features))\n",
        "    conv5 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs5)\n",
        "    bn5 = BatchNormalization()(conv5)\n",
        "    pool5 = GlobalAveragePooling1D()(bn5)\n",
        "    flat5 = Flatten()(pool5)\n",
        "\n",
        "    # merge\n",
        "    merged = concatenate([flat1,flat2,flat3,flat4,flat5])\n",
        "\n",
        "    # interpretation\n",
        "    dense1 = Dense(16, activation='relu')(merged) #16\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3, inputs4,inputs5], outputs=outputs)\n",
        "\n",
        "    # save a plot of the model\n",
        "    # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QGq3c2IoZr_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping Signal Sequences into Frames with the Sliding Window Method"
      ],
      "metadata": {
        "id": "iUKEjTJtZ5wN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNuY5asp3lgi"
      },
      "outputs": [],
      "source": [
        "def get_frames(X, Y):\n",
        "  \n",
        "  N_FEATURES = 6\n",
        "\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for Subject_Trial_Number_Encoded in set(X.Subject_Trial_Number_Encoded):  #for each group\n",
        "    current_frame = X.loc[X.Subject_Trial_Number_Encoded == Subject_Trial_Number_Encoded]  #get the all the frames for that group\n",
        "    start_index = min(current_frame.index)\n",
        "    end_index = max(current_frame.index) + 1\n",
        "    frame_size = len(current_frame)\n",
        "\n",
        "    ax = X['x_accelero'].values[start_index: end_index] \n",
        "    ay = X['y_accelero'].values[start_index: end_index]\n",
        "    az = X['z_accelero'].values[start_index: end_index]\n",
        "    gx = X['x_gyro'].values[start_index: end_index] \n",
        "    gy = X['y_gyro'].values[start_index: end_index]\n",
        "    gz = X['z_gyro'].values[start_index: end_index]\n",
        "\n",
        "    # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(Y[start_index: end_index])[0][0]\n",
        "    \n",
        "    frames.append([ax, ay, az, gx, gy, gz])\n",
        "    labels.append(label)\n",
        "  # returns frames of samples, each sample(group) with three features, each feature with n timesteps 28*3*1729 i.e. groups/samples * features * timesteps\n",
        "\n",
        "  # As the frame size differes for each group, the frames are padded\n",
        "  padded_frames = []\n",
        "  for row in frames:\n",
        "    shape = np.shape(row)\n",
        "    padded_array = np.zeros((6, 3000)) \n",
        "    padded_array[:shape[0],:shape[1]] = row\n",
        "    padded_frames.append(padded_array)\n",
        "\n",
        "  # return exactly the same shape as above but padded so each group has same number of steps - might not need this for this experiment! *CHECK*\n",
        "\n",
        "  # Bring the segments into a better shape\n",
        "  reshaped_padded_frames = np.transpose(padded_frames, (0, 2, 1))\n",
        "  reshaped_labels = np.asarray(labels)\n",
        "  \n",
        "  # finally converts the  28*3*1729  -> 28*1729*3 i.e. groups/samples * timestep * features\n",
        "  return reshaped_padded_frames, reshaped_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "vH0oBEvSZ884"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mewRfG8O3lkQ"
      },
      "outputs": [],
      "source": [
        "def get_scores(sensor_position_number, activity_number, full_gd_df): \n",
        "\n",
        "  partial_df = final_df.drop(columns=[0,  \n",
        "                                    'subject_name', 'trial_number','timestamp', \n",
        "                                    'Age', 'Age Group', 'Gender', 'Weight', 'Height', 'BMI',\n",
        "                                    'Subject_Trial_Sensor_Activity_Number', 'Subject_Trial_Sensor_Number']) #DROP MORE COLUMNS\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  partial_df['Subject_Trial_Number_Encoded'] = le.fit_transform(partial_df['Subject_Trial_Number'])\n",
        "  partial_df = partial_df.drop(columns=['Subject_Trial_Number'])\n",
        "  partial_df_original = partial_df[partial_df[\"activity\"] == activity_number]\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 1\n",
        "  partial_df = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[0]]\n",
        "  partial_df = partial_df.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train = partial_df[partial_df['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test = partial_df[partial_df['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN = partial_df_train[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN = partial_df_train['Gender Code']\n",
        "  X_TRAIN = X_TRAIN.to_numpy() # for LOGO\n",
        "  y_TRAIN = y_TRAIN.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST = partial_df_test[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST = partial_df_test['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test, reshaped_y_test = get_frames(X_TEST, y_TEST) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 2\n",
        "  partial_df_2 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[1]]\n",
        "  partial_df_2 = partial_df_2.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_2 = partial_df_2[partial_df_2['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_2 = partial_df_train_2[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_2 = partial_df_train_2['Gender Code']\n",
        "  X_TRAIN_2 = X_TRAIN_2.to_numpy() # for LOGO\n",
        "  y_TRAIN_2 = y_TRAIN_2.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_2 = partial_df_test_2[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_2 = partial_df_test_2['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_2, reshaped_y_test_2 = get_frames(X_TEST_2, y_TEST_2) # convert test data to frames\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 3\n",
        "  partial_df_3 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[2]]\n",
        "  partial_df_3 = partial_df_3.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_3 = partial_df_3[partial_df_3['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_3 = partial_df_3[partial_df_3['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_3 = partial_df_train_3[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_3 = partial_df_train_3['Gender Code']\n",
        "  X_TRAIN_3 = X_TRAIN_3.to_numpy() # for LOGO\n",
        "  y_TRAIN_3 = y_TRAIN_3.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_3 = partial_df_test_3[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_3 = partial_df_test_3['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_3, reshaped_y_test_3 = get_frames(X_TEST_3, y_TEST_3) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 4\n",
        "  partial_df_4 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[3]]\n",
        "  partial_df_4 = partial_df_4.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_4[['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_4 = partial_df_4[partial_df_4['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_4 = partial_df_4[partial_df_4['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_4 = partial_df_train_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_4 = partial_df_train_4['Gender Code']\n",
        "  X_TRAIN_4 = X_TRAIN_4.to_numpy() # for LOGO\n",
        "  y_TRAIN_4 = y_TRAIN_4.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_4 = partial_df_test_4[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_4 = partial_df_test_4['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_4, reshaped_y_test_4 = get_frames(X_TEST_4, y_TEST_4) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Get data for sensor 5\n",
        "  partial_df_5 = partial_df_original[partial_df_original[\"sensor_position\"] == sensor_position_number[4]]\n",
        "  partial_df_5 = partial_df_5.drop(columns=['sensor_position', 'activity']).reset_index(drop=True) #this resets index each time for different sensor and activity combinations\n",
        "\n",
        "  # Standardise\n",
        "  scaler = StandardScaler()\n",
        "  partial_df_5[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']] = scaler.fit_transform(partial_df_5[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro']])\n",
        "\n",
        "  # Partition Train and Test Data\n",
        "  partial_df_train_5 = partial_df_5[partial_df_5['Subject_Trial_Number_Encoded'] < 79].reset_index(drop=True) # check value!\n",
        "  partial_df_test_5 = partial_df_5[partial_df_5['Subject_Trial_Number_Encoded'] >= 79].reset_index(drop=True) \n",
        "\n",
        "  # Define TRAIN X and y variables\n",
        "  X_TRAIN_5 = partial_df_train_5[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']]\n",
        "  y_TRAIN_5 = partial_df_train_5['Gender Code']\n",
        "  X_TRAIN_5 = X_TRAIN_5.to_numpy() # for LOGO\n",
        "  y_TRAIN_5 = y_TRAIN_5.to_numpy() # for LOGO\n",
        "\n",
        "  # Define TEST X and y variables\n",
        "  X_TEST_5 = partial_df_test_5[['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded']].reset_index(drop=True)\n",
        "  y_TEST_5 = partial_df_test_5['Gender Code'].reset_index(drop=True)\n",
        "\n",
        "  reshaped_X_test_5, reshaped_y_test_5 = get_frames(X_TEST_5, y_TEST_5) # convert test data to frames\n",
        "  \n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  groups = partial_df_train['Subject_Trial_Number_Encoded']\n",
        "  logo = LeaveOneGroupOut()\n",
        "  split_number = logo.get_n_splits(X_TRAIN, y_TRAIN, groups)\n",
        "  groups = groups.to_numpy()\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "\n",
        "  for train_ix, val_ix in logo.split(X_TRAIN, y_TRAIN, groups):\n",
        "      # split data\n",
        "      X_train, X_val = X_TRAIN[train_ix, :], X_TRAIN[val_ix, :]\n",
        "      y_train, y_val = y_TRAIN[train_ix], y_TRAIN[val_ix] \n",
        "\n",
        "      X_train = pd.DataFrame(data = X_train, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      X_val = pd.DataFrame(data = X_val, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      y_train = pd.DataFrame(data = y_train)\n",
        "      y_val = pd.DataFrame(data = y_val) \n",
        "\n",
        "      reshaped_X_train, reshaped_y_train = get_frames(X_train, y_train) \n",
        "      reshaped_X_val, reshaped_y_val = get_frames(X_val, y_val) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_2, X_val_2 = X_TRAIN_2[train_ix, :], X_TRAIN_2[val_ix, :]\n",
        "      y_train_2, y_val_2 = y_TRAIN_2[train_ix], y_TRAIN_2[val_ix] \n",
        "\n",
        "      X_train_2 = pd.DataFrame(data = X_train_2, columns = ['x_accelero', 'y_accelero', 'z_accelero','x_gyro', 'y_gyro', 'z_gyro', 'Subject_Trial_Number_Encoded'])\n",
        "      X_val_2 = pd.DataFrame(data = X_val_2, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      y_train_2 = pd.DataFrame(data = y_train_2)\n",
        "      y_val_2 = pd.DataFrame(data = y_val_2) \n",
        "\n",
        "      reshaped_X_train_2, reshaped_y_train_2 = get_frames(X_train_2, y_train_2) \n",
        "      reshaped_X_val_2, reshaped_y_val_2 = get_frames(X_val_2, y_val_2) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_3, X_val_3 = X_TRAIN_3[train_ix, :], X_TRAIN_3[val_ix, :]\n",
        "      y_train_3, y_val_3 = y_TRAIN_3[train_ix], y_TRAIN_3[val_ix] \n",
        "\n",
        "      X_train_3 = pd.DataFrame(data = X_train_3, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      X_val_3 = pd.DataFrame(data = X_val_3, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      y_train_3 = pd.DataFrame(data = y_train_3)\n",
        "      y_val_3 = pd.DataFrame(data = y_val_3) \n",
        "\n",
        "      reshaped_X_train_3, reshaped_y_train_3 = get_frames(X_train_3, y_train_3) \n",
        "      reshaped_X_val_3, reshaped_y_val_3 = get_frames(X_val_3, y_val_3) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "       # split data\n",
        "      X_train_4, X_val_4 = X_TRAIN_4[train_ix, :], X_TRAIN_4[val_ix, :]\n",
        "      y_train_4, y_val_4 = y_TRAIN_4[train_ix], y_TRAIN_4[val_ix] \n",
        "\n",
        "      X_train_4 = pd.DataFrame(data = X_train_4, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      X_val_4 = pd.DataFrame(data = X_val_4, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      y_train_4 = pd.DataFrame(data = y_train_4)\n",
        "      y_val_4 = pd.DataFrame(data = y_val_4) \n",
        "\n",
        "      reshaped_X_train_4, reshaped_y_train_4 = get_frames(X_train_4, y_train_4) \n",
        "      reshaped_X_val_4, reshaped_y_val_4 = get_frames(X_val_4, y_val_4) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # split data\n",
        "      X_train_5, X_val_5 = X_TRAIN_5[train_ix, :], X_TRAIN_5[val_ix, :]\n",
        "      y_train_5, y_val_5 = y_TRAIN_5[train_ix], y_TRAIN_5[val_ix] \n",
        "\n",
        "      X_train_5 = pd.DataFrame(data = X_train_5, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      X_val_5 = pd.DataFrame(data = X_val_5, columns = ['x_accelero', 'y_accelero', 'z_accelero', 'x_gyro', 'y_gyro', 'z_gyro','Subject_Trial_Number_Encoded'])\n",
        "      y_train_5 = pd.DataFrame(data = y_train_5)\n",
        "      y_val_5 = pd.DataFrame(data = y_val_5) \n",
        "\n",
        "      reshaped_X_train_5, reshaped_y_train_5 = get_frames(X_train_5, y_train_5) \n",
        "      reshaped_X_val_5, reshaped_y_val_5 = get_frames(X_val_5, y_val_5) \n",
        "\n",
        "      #-------------------------------------------------------------------------------------------\n",
        "\n",
        "      # defining some input variables\n",
        "      n_timesteps, n_features, n_outputs = reshaped_X_train.shape[1], reshaped_X_val.shape[2], reshaped_y_train.shape[1]\n",
        "\n",
        "      # getting the model\n",
        "      model = cnn_model_creation(n_timesteps, n_features)\n",
        "\n",
        "      # fit model\n",
        "      history = model.fit([reshaped_X_train,reshaped_X_train_2,reshaped_X_train_3,reshaped_X_train_4,reshaped_X_train_5], reshaped_y_train,\n",
        "                epochs=50,\n",
        "                verbose=0,\n",
        "                callbacks=[lr_callback],\n",
        "                validation_data=([reshaped_X_val,reshaped_X_val_2,reshaped_X_val_3,reshaped_X_val_4,reshaped_X_val_5], reshaped_y_val)) # Chnage number of epochs!\n",
        "      \n",
        "      test_scores = model.evaluate([reshaped_X_test, reshaped_X_test_2,reshaped_X_test_3,reshaped_X_test_4,reshaped_X_test_5],reshaped_y_test, verbose=0)\n",
        "      acc_per_fold.append(test_scores[1] * 100)\n",
        "      loss_per_fold.append(test_scores[0])\n",
        "  mean_accuracy = np.mean(acc_per_fold)\n",
        "  mean_std = np.std(acc_per_fold)\n",
        "  mean_loss = np.mean(loss_per_fold)\n",
        "  print('Sensor:', sensor_position_number, 'Activity:', activity_number)\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  return mean_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the Train Function"
      ],
      "metadata": {
        "id": "DUbuuPFWaFR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUUTetRP3lwT"
      },
      "outputs": [],
      "source": [
        "test_scores = []\n",
        "activity_list = [1, 2, 3, 4, 5, 6, 7]\n",
        "sensor_list = [\n",
        "               [1,2,3,4,5]\n",
        "               ]\n",
        "for activity in activity_list:\n",
        "  for sensor in sensor_list:\n",
        "    mean_accuracy = get_scores(sensor, activity, final_df)     \n",
        "    test_scores.append((activity, sensor, mean_accuracy))\n",
        "\n",
        "test_scores"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}